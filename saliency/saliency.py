import pickle
import numpy as np
import skimage.measure
import math
import quaternion
import matplotlib.pyplot as plt
import os



RGB_to_YCbCr = np.ndarray((3,3), dtype=float)
RGB_to_YCbCr[0,:] = [0.257, 0.504, 0.098]
RGB_to_YCbCr[1,:] = [-0.148, -0.291, 0.439]
RGB_to_YCbCr[2,:] = [0.439, -0.368, -0.071]
# agent_oreo.get_sensor_observation produces a ndarray 512x512x3 ordered as BGR as required by inshow


''' Processing instruction that assumes the use of colab to run Deep Gaze II.
    For processing Deep Gaze output,
    1.  - A start image file scene^timestamp has a unique timestamp and view of the oreo habitat scene as seen by oreo
        (van-gogh-room.glb^2021-05-09-22-37)
    2.  - create a sal_object by invoking the process_image with file location of the starting image.
        It has the start left and right images but no saliency map for them
    3.  Use Deep Gaze II to produce the saliency map for the starting image L R images
    4.  Invoke the save_salmap(ndarray of 2D saliency values) to create "-sal" file containing just the saliency map
    5.  call process_saliency without -sal file if you are doing it right after save_salmap
        This processes the file, generates 10 points and contains the images of the processed salmap files
    6.  calling save_all saves a "-sal-processed"
    7.  Invoking capture_images_for_salpoints("-sal-processed" file) in Habitat_oreo object generates an image-ensemble file.
        that ends with "-sal-processed-images". It is generated by using the get_salpoints("-sal-processed file") method
        in saliency.py to iterate through the points and capture images of the same oreo habitat scene with the gaze vectors
        in the direction of the point. At present the initial L and R images is not fixated.
    8.  create a sal-ensemble object ("-sal-processed-images" file) and use Deep Gaze to obtain salmap for each image.
    9.  save_sal_ensemble(left_map_list, right_map_list) creates  "-sal-processed-images-sal-ensemble" file containing
        all the saliency maps.

'''


def map_pixels(R1, R2, focal_distance, width, height):
    '''
    Reference for rotation is Habitat World Coordinates
    :param R1 Rotation in quaterion from WCS to start image whose pixel locations are known
    :param R2 Rotation in quaterion from WCS to any other image frame
    :param focal_distance: the frame z coordinate is equal to -focal_distance
    :param width: sensor width pixels
    :param height: sensor height pixels
    :returns common pixels, pixels only in start image and pixels only in the new frame
    '''

    R = R2.inverse()*R1     #Rotation from any image frame to start image frame
    w = width/2
    h = height/2

    count = 0
    common_pixels = []
    only_start_image_pixels = []
    other_image_exclusion = []
    other_image_pixels =[]
    for xpos in range(width):               # xpos in start image - column
        for ypos in range(height):          # ypos in start image - row
            # x, y for origin at the center of the frame and computing the unit vector
            x = xpos - w
            y = h - ypos
            v = np.array([x, y, -focal_distance])
            uvector = v / np.linalg.norm(v)

            # rotate the uvector
            new_vector = quaternion.as_rotation_matrix(R).dot(uvector.T)

            ux = new_vector[0]
            uy = new_vector[1]
            uz = new_vector[2]
            # calculate angles that the unit vector makes with z axis and with xz plane
            uxz = np.sqrt(ux * ux + uz * uz)
            theta = np.arcsin(ux / uxz)  # z is never zero, theta is the angle wrt to y
            phi = np.arcsin(uy)  # x is the angle wrt to z
            # compute x,y (z = -focal length)
            xval = focal_distance * np.tan(theta)
            yval = focal_distance * np.tan(phi)
            # convert to top left origin
            xnew = xval + w
            ynew = h - yval
            if 0 <= xnew <= width and 0<= ynew <= height:
                corresponding_pixel = (xnew, ynew)
                common_pixels.append([(xpos, ypos), corresponding_pixel])
                other_image_exclusion.append(corresponding_pixel)
                count +=1
            else:
                only_start_image_pixels.append((xpos, ypos))

    for xp in range(width):          # xp in frame 2 - column
        for yp in range(height):     # yp in frame 2 - row
            if (xp,yp) not in other_image_exclusion:
                other_image_pixels.append((xp,yp))

    return common_pixels, only_start_image_pixels, other_image_pixels


def scale_image(some_image, new_max = 150):
    vmin = some_image.min()
    vmax = some_image.max()
    some_image = (some_image - vmin) * new_max / (vmax - vmin)
    return some_image


def find_max_and_index(array_2d):
    #result = np.where(array_2d == np.amax(array_2d))
    result = np.nonzero(array_2d == np.amax(array_2d))
    r = result[0][0]
    c = result[1][0]
    return np.amax(array_2d), r, c


def get_salpoints(processed_sal_file):
    '''
    :parameter processed_sal_file - A processed saliency file, name ending with "-sal-processed".
    It would contain a maximum of 10 salient points identified in the saliency heat map.
    This function can be used to read such a file.
    :returns agent orientation, agent Position, robot_head_neck_rotation,
    left_image, lefteye Rotation, list of x,y points, right_image, righteye Rotation, list of x,y points
    '''
    try:
        with open(processed_sal_file, "rb") as f:
            saldata = pickle.load(f)
            sal_pL = []
            sal_pR = []
            for i in saldata[4][0]:
                sal_pL.append(i[2:])
            for i in saldata[4][1]:
                sal_pR.append(i[2:])

            return saldata[6], saldata[7], saldata[9], saldata[0][0], saldata[8][0], sal_pL, saldata[0][1], \
                   saldata[8][1], sal_pR
    except:
        print(f"Failure: To read saliency processed datafile {processed_sal_file}")
        return None

def read_pickled_file(sfilename, filetype=" "):
    '''
    :parameter - sfilename is a filename
    '''
    try:
        with open(sfilename, "rb") as f:
            pickled_data = pickle.load(f)
            return pickled_data
    except:
        print(f"Failure: To read {filetype} file {sfilename}")
        return None

def compute_pixel_in_current_frame(R1, R2, pixels_in_previous_frame, focal_distance, width, height):
    '''
    Reference for rotation is Habitat World Coordinates
    :param R1 Rotation in quaterion from WCS to previous sensor(eye) frame
    :param R2 Rotation in quaterion from WCS to current sensor(eye) frame
    :param pixels_in_previous_frame: List of x,y positions of pixel in the previous frame
    :param focal_distance: the frame z coordinate is equal to -focal_distance
    :param width: sensor width pixels
    :param height: sensor height pixels
    :returns List of x, y positions in the current frame
    '''

    R = R2.inverse() * R1  # Rotation from current frame to previous frame
    w = width / 2
    h = height / 2
    new_list = []
    for i in pixels_in_previous_frame:  # i should (x,y)
        # shifting based on an origin at the center of the frame and computing the unit vector
        x = i[0] - w
        y = h - i[1]
        v = np.array([x, y, -focal_distance])
        uvector = v / np.linalg.norm(v)

        # rotate the uvector
        new_vector = quaternion.as_rotation_matrix(R).dot(uvector.T)

        ux = new_vector[0]
        uy = new_vector[1]
        uz = new_vector[2]
        # calculate angles that the unit vector makes with z axis and with xz plane
        uxz = np.sqrt(ux * ux + uz * uz)
        theta = np.arcsin(ux / uxz)  # z is never zero, theta is the rotation angle about y-axis - yaw angle
        phi = np.arcsin(uy)  # x is the angle about x - pitch angle
        # compute x,y (z = -focal length)
        xval = focal_distance * np.tan(theta)
        yval = focal_distance * np.tan(phi)
        # convert to top left origin
        xn = math.floor(xval + w)
        yn = math.floor(h - yval)
        #if xn <= width and yn <= height:    # logic is suspect;
        if 0 <= xn <= width and 0<= yn <= height:  # logic is suspect;
            pos = (xn, yn)
            #combo = (i, pos)
            new_list.append(pos)
        else:
            new_list.append(None)
            print(f"point {i}in old frame is outside the new frame at {xn},{yn}")
    return new_list


class process_image(object):
    '''
    Since salmap comes from Deep Gaze II through collab, saving the salmap is not in the __init__.
    Creating a process image object will allow us to run Deep Gaze II for two images self.imageL and self.imageR
    individually and save their saliency maps without running habitat-sim in collab - collab notebook.
    Once created, use save_salmap to save the saliency maps of from the left and right images to create a -sal file.
    Its methods can also be used to create a "-processed" which will contain image, salmap, centerpoints, and
    rotation information for both images.
    '''

    def __init__(self, image_info_file, my_block = 16, total_points = 10, pixel_max = 150):
        self.fname = image_info_file
        try:
            with open(image_info_file, "rb") as f:
                scene_data = pickle.load(f)
        except IOError as e:
            print("Failure: Loading Image pickle file {}".format(image_info_file))
            exit(1)

        '''    
        scene_data
            0 - Agent orn
            1 - Agent position
            2 - robot_head_neck_rotation
            3 - lefteye orientation
            4 - righteye orientation
            5 - left_sensor.resolution
            6 - left_sensor_hfov
            7 - focal_distance
            8 - images[0 = left, 1 = right, 2 = depth optionally set]
        '''
        self.imageL = scene_data[8][0][:, :, ::-1]  # Left image
        self.imageR = scene_data[8][1][:, :, ::-1]  # Right image
        self.lefteye = scene_data[3]    # Left sensor rotation
        self.righteye = scene_data[4]   # right sensor rotation
        self.agent_orn = scene_data[0]  # Agent orn
        self. agent_pos = scene_data[1]  # Agent position
        self.hfov = scene_data[6]
        self.focal_distance = scene_data[7]
        self.robot_head_neck_rotation = scene_data[2] #head-neck rotation that is buried into the Agent orn.
        self.block_dim = my_block
        self.num_points = total_points
        self.new_max = pixel_max
        #Saliency - left image
        self.salmapL = None
        self.reduced_salmapL = None
        self.recreated_salmapL = None
        self.reduced_sal_pointsL = []
        self.center_pointsL = []
        #Saliency - right image
        self.salmapR = None
        self.reduced_salmapR = None
        self.recreated_salmapR = None
        self.reduced_sal_pointsR = []
        self.center_pointsR = []

        head, tail = os.path.split(image_info_file)
        results_folder = head + '/results'
        if not os.path.exists(results_folder):
            os.makedirs(results_folder)
        self.output_filename = results_folder + '/' + tail + "-sal-processed"
        self.salmap_filename = results_folder + '/' + tail + "-sal"

    def load_salmap(self,salval, which_eye = "left"):
        '''
        Loads the individual saliency heat map for each eye into the process object for an image file
        created by agent_oreo.py
        :param salval: saliency map as numpy ndarray, from Deep Gaze II output
        Use this to save both the right and left image saliency maps from Deep Gaze II output
        '''

        if which_eye == "right":
            self.salmapR = salval
        else:
            self.salmapL = salval

    def save_salmap(self):
        '''
        Use this to save the right and left saliency values from Deep Gaze II for a given right and left image pair
        '''

        both_salmap = [self.salmapL, self.salmapL]
        try:
            with open(self.salmap_filename, "wb") as f:
                pickle.dump(both_salmap, f)
                return self.salmap_filename
        except:
            print(f"Failure: To open and save saliency file {self.salmap_filename}")


    def process_saliency(self, sal_file = None):
        '''
        :parameter block_dim - size reduction of original image for identifying salient points
        :parameter num_points - number of salient points to be identified.
        It creates reduced_salmap, recreated_salmap, and list of salient points.
        '''
        if sal_file is not None:     # salmap available as a file to run outside of colab
            try:
                with open(sal_file, "rb") as f:
                    salmaps = pickle.load(f)
                    self.salmapL = salmaps[0]
                    self.salmapR = salmaps[1]
            except IOError as e:
                print(f"Failure: Loading pickle saliency map file {sal_file}")
                return

        # This portion would also work without sal_file immediately after saving the Deep Gaze II.
        if self.salmapL is None and self.salmapR is None:
            print(print(f"Failure: Saliency numpy ndarray is empty"))
            return
        else:
            # self.salmap[0] = left saliency map and self.salmap[1] = right saliency map
            # Macular angle is 18 out of HFOV (120); d should be 5
            # We ignore (d-1)/2 pixels on horizontal and vertical directions of a salient pixel
            # When dim = 16x16 for map reduction, d=5 means 2 pixels on each side, 32 pixels in 512 scale
            # Each red. image pixel = 16 regular approximately 3.5 degrees
            hfov = np.degrees(self.hfov)
            self.d = math.ceil((18.0 * self.salmapL.shape[0]) / (hfov * self.block_dim))
            # print(f"The dimension d = {self.d}")
            # compute salient points list
            f = int((self.d - 1) / 2)
            if self.salmapL is not None:
                self.compute_points_and_maps(f)
            if self.salmapR is not None:
                self.compute_points_and_maps(f,"right")


    def compute_points_and_maps(self, f, which_map = "left"):
        # Macular angle is 18 out of HFOV (120); d should be 5
        # We ignore (d-1)/2 pixels on horizontal and vertical directions of a salient pixel
        # When dim = 16x16 for map reduction, d=5 means 2 pixels on each side, 32 pixels in 512 scale
        # Each red. image pixel = 16 regular approximately 3.5 degrees

        if which_map == "right":
            raw_salmap = self.salmapR
        else:
            raw_salmap = self.salmapL

        reduced_salmap = skimage.measure.block_reduce(raw_salmap, block_size=(self.block_dim,
                                                                                      self.block_dim), func=np.amax)
        reduced_salmap = scale_image(reduced_salmap)
        reduced_sal_points = []
        for i in range(self.num_points):
            val, r, c = find_max_and_index(reduced_salmap)
            reduced_sal_points.append((val, r, c))
            reduced_salmap[r - f:r + f + 1, c - f:c + f + 1] = 0  # zero_around_macular_center

        for i in range(self.num_points):
            reduced_salmap[reduced_sal_points[i][1], reduced_sal_points[i][2]] = 255 - i * 10

        new_sal_image = np.copy(raw_salmap)
        vmin = new_sal_image.min()
        vmax = new_sal_image.max()
        recreated_salmap = (new_sal_image - vmin) * self.new_max / (vmax - vmin)

        count = 0
        center_points = []
        for scaled_val, r, c in reduced_sal_points:
            start_r = r * self.block_dim
            start_c = c * self.block_dim
            '''
            recreated_image[start_r:start_r+block_size,start_c:start_c+block_size] = \
                recreated_image[start_r:start_r + block_size, start_c:start_c + block_size]/2
            '''
            val = 255 - 10 * count
            recreated_salmap[start_r:start_r + self.block_dim, start_c:start_c + self.block_dim] = val
            true_val = raw_salmap[start_r + 8, start_c + 8]
            center_points.append((true_val, scaled_val, start_r + 8, start_c + 8))
            count += 1

        if which_map == "left":
            self.reduced_salmapL = reduced_salmap
            self.recreated_salmapL = recreated_salmap
            self.center_pointsL = center_points
            self.reduced_sal_pointsL = reduced_sal_points

        else:
            self.reduced_salmapR = reduced_salmap
            self.recreated_salmapR = recreated_salmap
            self.center_pointsR = center_points
            self.reduced_sal_pointsR = reduced_sal_points



    def save_all(self):
        '''
        After processing, call this to save a -sal-processed file which contains both images, salicency heat maps,
        list of salient points, and the associated posn orientation of the eye sensors.
        '''
        all = []
        image = [self.imageL, self.imageR]
        all.append(image)
        salmap = [self.salmapL,self.salmapR]
        all.append(salmap)
        reduced_salmap = [self.reduced_salmapL,self.reduced_salmapR]
        all.append(reduced_salmap)
        recreated_salmap = [self.recreated_salmapL, self.recreated_salmapR]
        all.append(recreated_salmap)
        center_points = [self.center_pointsL, self.center_pointsR]
        all.append(center_points)
        all.append(self.focal_distance)
        all.append(self.agent_orn)
        all.append(self.agent_pos)
        rotation = [self.lefteye, self.righteye]
        all.append(rotation)
        all.append(self.robot_head_neck_rotation)

        try:
            with open(self.output_filename, "wb") as f:
                pickle.dump(all, f)
                print(f"Saved processed saliency file {self.output_filename}")
        except:
            print(f"Failure: To save saliency file {self.output_filename}")


    def read_sal_datafile(self):
        '''
        reads the processed file saved by process_image object
        :return: List or None.
            0 - image L, R
            1 - salmap L, R
            2 - reduced_salmap L, R
            3 - recreated_salmap L, R
            4 - center_points L , R
            5 - focal_distance
            6 - agent_orn
            7 - agent_pos
            8 - rotation L, R
            9 - robot_head_neck_rotation
        '''
        try:
            with open(self.output_filename, "rb") as f:
                saldata = pickle.load(f)
                return saldata
        except:
            print(f"Failure: To read saliency processed datafile {self.output_filename}")
            return None

'''
    Use sal_ensemble class and Deep Gaze II to create the ensemble of saliency heatmaps of the image ensemble.
    1 - create a sal_ensemble object by passing the file ending with "-sal-processed-images". This file was created when the
    initial image saliency is used to identify salient points and saccade the sensor to capture a set of images.
    2 - get the left and right images as lists by invoking get_images method.
    3 - Use Deep Gaze II to get the saliency maps for each. See (Get_salmap_for_images.ipynb)
    4 - Two lists of saliency maps for the left and right set of images is passed while invoking save_sal_ensemble method
    5 - save_sal_ensemble creates the -sal-processed-images-sal-ensemble
'''

class sal_ensemble(object):

    def __init__(self, image_ensemble_file):
        try:
            with open(image_ensemble_file, "rb") as f:
                data = pickle.load(f)
        except IOError as e:
            print("Failure: Loading pickle file {}".format(image_ensemble_file))
            exit(1)

        self.imgL_data = data[0]
        self.imgR_data = data[1]
        head, tail = os.path.split(image_ensemble_file)
        results_folder = head + '/results'
        if not os.path.exists(results_folder):
            os.makedirs(results_folder)
        self.sal_ensemble_filename = results_folder + '/' + tail + "-sal-ensemble"

    def get_images(self):

        left_images = []
        for i in self.imgL_data:
            if i[1] is not None:
                left_images.append(i[1][4])
            else:
                left_images.append(None)

        right_images = []
        for i in self.imgR_data:
            if i[1] is not None:
                right_images.append(i[1][4])
            else:
                right_images.append(None)
        return left_images, right_images

    def save_sal_ensemble(self, left_salmaps, right_salmaps):
        output = [left_salmaps,right_salmaps]
        try:
            with open(self.sal_ensemble_filename, "wb") as f:
                pickle.dump(output, f)
                print(f"Saved new saliency file {self.sal_ensemble_filename}")
        except IOError as e:
            print(f"Failure: To open/write file {self.sal_ensemble_filename}")


'''
    class fixation_comparison takes 3 files:
    a)  start_processed - filename ending with -sal-processed
    b)  image_ensemble - filename ending with -sal-processed-images"
    c)  sal_ensemble - sal ensemble ending with sal-processed-images-sal-ensemble"
    
    Create a fixation_comparision object.
    Invoke build_point_map() - creates a list of list of the salient points A list of 11 lists each of which has 10 points
    Invoke compute_sal_variations - to create a list of list of the saliency values of the points_map.
'''

class fixation_comparison(object):

    def __init__(self, start_image_processed_filename, image_ensemble_filename, sal_ensemble_filename):
        '''
        Corresponding pixels for fixations requires Starting_R, current_R, ref_point_list, focal_distance, w, h
        start_image_processed_file contains the following list
        [0] - images - [L, R]
        [1] - salmaps - [L, R]
        [3] - recreated_salmap [L, R]
        [4] - List of points [L, R]
        [5] - focal_distance
        [6] - agent_orn
        [7] - agent_pos
        [8] - rotation [lefteye, righteye]
        [9] - robot_head_neck_rotation
        Image ensemble is a list = [L,R] L/R is [gaze_point, [aorn, apos, agent_head_neck_rotation, l_sensor_orn, image]]
        Sal ensemble is a list =[L,R] l/R is a list of salmaps
        '''

        self.start_data = read_pickled_file(start_image_processed_filename, "processed start image file")
        if self.start_data is None:
             exit(1)
        d = start_image_processed_filename.find("-sal-processed")
        self.fixation_variations_filename = start_image_processed_filename[0:d] + "-fixation_variations"

        self.start_sensor_rotation = self.start_data[8]     #Reference Rotation [L,R] of start image
        self.focal_distance = self.start_data[5]
        self.salmap_dim = self.start_data[1][0].shape       #left salmap is used to get (w,h)

        self.fixation_pointsL = []
        # 10 fixation points - from the principle position
        for i in self.start_data[4][0]:
            self.fixation_pointsL.append((i[2:]))
        self.fixation_pointsR = []
        for i in self.start_data[4][1]:
            self.fixation_pointsR.append((i[2:]))

        self.image_ensemble = read_pickled_file(image_ensemble_filename, "image ensemble file")
        if self.image_ensemble is None:
            exit(1)
        self.fixation_rotationsL = []
        self.imagesL = []
        # 10 points in original image = 10 images
        # 10 rotations one for every image
        for i in self.image_ensemble[0]:                    #Left
           # i[0] = pixel (x,y)
           # i[1] = [aorn, apos, oreo_in_habitat.agent_head_neck_rotation, l_sensor_orn, image]
            if i[1] is None:                                #No data for the gaze point
                self.fixation_rotationsL.append(None)
                self.imagesL.append(None)
            else:
                self.fixation_rotationsL.append(i[1][3])     #sensor rotation
                self.imagesL.append(i[1][4])                 #image
        self.fixation_rotationsR = []
        self.imagesR = []
        for i in self.image_ensemble[1]:                    #right
            if i[1] is None:                                #None data for the gaze point
                self.fixation_rotationsR.append(None)
                self.imagesR.append(None)
            else:
                self.fixation_rotationsR.append(i[1][3])     #sensor rotation
                self.imagesR.append(i[1][4])
        # Now 11 images
        self.imagesL.insert(0,self.start_data[0][0])         #inserting the start images to the 0th position
        self.imagesR.insert(0, self.start_data[0][1])

        self.sal_ensemble = read_pickled_file(sal_ensemble_filename, "saliency ensemble file")
        if self.sal_ensemble is None:
            exit(1)
        # left saliency heatmap = self.sal_ensemble[0], right saliency heatmap = self.sal_ensemble[1]
        self.sal_ensemble[0].insert(0,self.start_data[1][0])    #salmap of the Left start image inserted to 0th position
        self.sal_ensemble[1].insert(0, self.start_data[1][1])   #salmap of the right start image inserted to 0th position

        # Expecting to fillup the following using methods for the object
        # The locations of the fixation in each image will be in the point_map 
        self.point_mapL = []            # Will contain 11 lists (1 per image) - Each a list of 10 pixel coordinates
        self.point_mapR = []            
        self.sal_vals_from_fixationsL = []   # 11 lists, each a list of 10 saliency values at the pixels
        self.sal_vals_from_fixationsR = []  
        self.imagesL_avg = []
        self.imagesL_std = []
        self.imagesR_avg = []
        self.imagesR_std = []


    def build_point_map(self):
        '''
        It creates a n x n List of List. Each inner list is a set of pixel locations. The first list is the salient pixels
        location from the start image. The following lists present in top left 0,0 coordinate system, the locations of
        the salient points from the first list in each image as eye fixates in each of the salient points found in the
        start image.
        One point in each list (from 1 to 10) will have 256,256 point or close enough. This is the gaze direction for
        that image. A none list means the eye could not saccade to that point and a none value within a list means
        the point was outside the frame (h x w).
        '''

        self.point_mapL.append(self.fixation_pointsL)    #First list of 10 salient points from start left image
        for i in self.fixation_rotationsL:               #10 more lists - each with new locations for the 10 sal points
            if i is not None:
                point_set = compute_pixel_in_current_frame(self.start_sensor_rotation[0], i, self.fixation_pointsL,
                                                           self.focal_distance, self.salmap_dim[0], self.salmap_dim[1])
                self.point_mapL.append(point_set)
            else:
                my_list = [None] * len(self.fixation_pointsL)
                self.point_mapL.append(my_list)

        self.point_mapR.append(self.fixation_pointsR)    #A list of 10 salient points from start right image
        for i in self.fixation_rotationsR:
            if i is not None:
                point_set = compute_pixel_in_current_frame(self.start_sensor_rotation[0], i, self.fixation_pointsR,
                                                           self.focal_distance, self.salmap_dim[0], self.salmap_dim[1])
                self.point_mapR.append(point_set)
            else:
                my_list = [None] * len(self.fixation_pointsR)
                self.point_mapR.append(my_list)
        return


    def compute_sal_variations(self):

        for i, point_list in enumerate(self.point_mapL):    # i from 0 to 10, a total of 11 
            sal_values =[]
            for loc in point_list:
                if loc is None:
                    sal_values.append(np.nan)
                else:   #sal_ensemble[0] is left image saliency i corresponds to the ith image
                    sal_values.append(self.sal_ensemble[0][i][loc[0],loc[1]])
            self.sal_vals_from_fixationsL.append(sal_values)

        for i, point_list in enumerate(self.point_mapR):
            sal_values = []
            for loc in point_list:
                if loc is None:
                    sal_values.append(np.nan)
                else:
                    sal_values.append(self.sal_ensemble[1][i][loc[0], loc[1]])
            self.sal_vals_from_fixationsR.append(sal_values)

        # re-arrange to have lists of values per point

        self.imagewise_variations_of_sal_pixelL = [[] for _ in range(len(self.fixation_pointsL))]
        self.imagewise_variations_of_sal_pixelR = [[] for _ in range(len(self.fixation_pointsR))]

        for i in self.sal_vals_from_fixationsL:
            for j, k in enumerate(i):
                self.imagewise_variations_of_sal_pixelL[j].append(k)

        for i in self.sal_vals_from_fixationsR:
            for j, k in enumerate(i):
                self.imagewise_variations_of_sal_pixelR[j].append(k)

        output = [self.sal_vals_from_fixationsL, self.imagewise_variations_of_sal_pixelL, self.sal_vals_from_fixationsR,
                  self.imagewise_variations_of_sal_pixelL]
        try:
            with open(self.fixation_variations_filename, "wb") as f:
                pickle.dump(output, f)
        except:
            print(f"Failure: To open and save file for saving saliency "
                  f"variations of fixations{self.fixation_variations_filename}")
        return


    def compute_image_stats(self):

        avg_I = []
        avg_Cb = []
        avg_Cr = []
        std_I = []
        std_Cb = []
        std_Cr = []
        for image in self.imagesL:
            if image is None:
                avg_I.append(np.nan)
                avg_Cb.append(np.nan)
                avg_Cr.append(np.nan)
                std_I.append(np.nan)
                std_Cb.append(np.nan)
                std_Cr.append(np.nan)
            else:
                intensity = 16 + (RGB_to_YCbCr[0, 0] * image[..., 2] + RGB_to_YCbCr[0, 1] * image[..., 1] + RGB_to_YCbCr[0, 2] * image[..., 0])
                chrome_blue = 128 + (RGB_to_YCbCr[1, 0] * image[..., 2] + RGB_to_YCbCr[1, 1] * image[..., 1] + RGB_to_YCbCr[1, 2] * image[..., 0])
                chrome_red = 128 + (RGB_to_YCbCr[2, 0] * image[..., 2] + RGB_to_YCbCr[2, 1] * image[..., 1] + RGB_to_YCbCr[2, 2] * image[..., 0])

                avg_I.append(np.average(intensity))
                avg_Cb.append(np.average(chrome_blue))
                avg_Cr.append(np.average(chrome_red))

                std_I.append(np.std(intensity))
                std_Cb.append(np.std(chrome_blue))
                std_Cr.append(np.std(chrome_red))

        self.imagesL_avg = [avg_I, avg_Cb, avg_Cr]
        self.imagesL_std = [std_I, std_Cb, std_Cr]

        avg_I = []
        avg_Cb = []
        avg_Cr = []
        std_I = []
        std_Cb = []
        std_Cr = []
        for image in self.imagesR:
            if image is None:
                avg_I.append(np.nan)
                avg_Cb.append(np.nan)
                avg_Cr.append(np.nan)
                std_I.append(np.nan)
                std_Cb.append(np.nan)
                std_Cr.append(np.nan)
            else:
                intensity = 16 + (RGB_to_YCbCr[0,0]*image[...,2] + RGB_to_YCbCr[0,1]*image[...,1] + RGB_to_YCbCr[0,2]*image[...,0])
                chrome_blue = 128 + (RGB_to_YCbCr[1,0]*image[...,2] + RGB_to_YCbCr[1,1]*image[...,1] + RGB_to_YCbCr[1,2]*image[...,0])
                chrome_red = 128 + (RGB_to_YCbCr[2,0]*image[...,2] + RGB_to_YCbCr[2,1]*image[...,1] + RGB_to_YCbCr[2,2]*image[...,0])

                avg_I.append(np.average(intensity))
                avg_Cb.append(np.average(chrome_blue))
                avg_Cr.append(np.average(chrome_red))

                std_I.append(np.std(intensity))
                std_Cb.append(np.std(chrome_blue))
                std_Cr.append(np.std(chrome_red))

        self.imagesR_avg = [avg_I, avg_Cb, avg_Cr]
        self.imagesR_std = [std_I, std_Cb, std_Cr]

        return


    def saliency_variation_vs_image_stats(self):
        ''':returns: avg_tuple[x] compares the variation of xth salient pixel gaze to gaze wrt to avg value of the image;
        similarly std_tupe compares the variation of xth salient pixel gaze to gaze wrt to std value of the image.
        '''
        avg_tuple = []
        std_tuple = []
        for j in self.imagewise_variations_of_sal_pixelL:
            for i in range(3):                                          # 0 - Intensity, 1 - CrB, 2 - CrR.
                temp = zip(self.imagesL_avg[i], j)                      # Zip pixel saliency with corresp. Image Avg
                sorted_list = sorted(temp, key=lambda t: t[0])          # sort by avg small to high
                avg_val = []
                s_val = []
                for k in sorted_list:
                    avg_val.append(k[0])
                    s_val.append(k[1])
                avg_tuple.append([avg_val, s_val])
                temp = zip(self.imagesL_std[i], j)                      # Zip pixel saliency with corresp. Image std
                sorted_list = sorted(temp, key=lambda t: t[0])
                std_val = []
                s_val = []
                for k in sorted_list:
                    std_val.append(k[0])
                    s_val.append(k[1])
                std_tuple.append([std_val, s_val])
        return avg_tuple, std_tuple


image_info_file = "./saliency_map/van-gogh-room.glb^2021-06-22-21-23-41"
salmap_file = "./saliency_map/van-gogh-room.glb^2021-06-22-21-23-41-sal"

if __name__ == "__main__":

    salval = read_pickled_file(salmap_file)
    my_sal_object = process_image(image_info_file, my_block=16, total_points=10, pixel_max=150)
    my_sal_object.load_salmap(salval[0])
    my_sal_object.load_salmap(salval[1], "right")
    my_salfile = my_sal_object.save_salmap()
    my_sal_object.process_saliency()
    my_sal_object.save_all()
    fig = plt.figure(figsize=(8, 8))
    r1c1 = fig.add_subplot(2, 2, 1)
    r1c2 = fig.add_subplot(2, 2, 2)
    r2c1 = fig.add_subplot(2, 2, 3)
    r2c2 = fig.add_subplot(2, 2, 4)
    r1c1.imshow(my_sal_object.imageL)
    r1c2.imshow(my_sal_object.salmapL)
    r2c1.imshow(my_sal_object.reduced_salmapL)
    r2c2.imshow(my_sal_object.recreated_salmapL)
    plt.show()

    image_ensemble = "./saliency_map/van-gogh-room.glb^2021-05-09-22-47-sal-processed-images"

    nrow = 2
    ncol = 5
    for root, dirs, files  in os.walk("./saliency_map/"):
        for filename in files:
            if "sal-processed-images" in filename and "sal-processed-images-" not in filename:
                my_image_ensemble = sal_ensemble("./saliency_map/"+ filename)
                l_images, r_images = my_image_ensemble.get_images()
                fig, ax = plt.subplots(nrow, ncol)
                for i,img in enumerate(l_images):
                    x = i // ncol
                    y = i % ncol
                    my_label = f"image {i}"
                    ax[x, y].set_title(my_label)
                    if img is not None:
                        ax[x,y].imshow(img)

        break


    start_processed = "./saliency_map/van-gogh-room.glb^2021-05-09-22-47-sal-processed"
    image_ensemble = "./saliency_map/van-gogh-room.glb^2021-05-09-22-47-sal-processed-images"
    sal_ensemble = "./saliency_map/van-gogh-room.glb^2021-05-09-22-47-sal-processed-images-sal-ensemble"
    '''
    start_processed = "./saliency_map/van-gogh-room.glb^2021-05-09-22-37-sal-processed"
    image_ensemble = "./saliency_map/van-gogh-room.glb^2021-05-09-22-37-sal-processed-images"
    sal_ensemble = "./saliency_map/van-gogh-room.glb^2021-05-09-22-37-sal-processed-images-sal-ensemble"
    '''
    my_comparison_object = fixation_comparison(start_processed, image_ensemble, sal_ensemble)
    my_comparison_object.build_point_map()
    my_comparison_object.compute_sal_variations()
    my_comparison_object.compute_image_stats()
    a, b = my_comparison_object.saliency_variation_vs_image_stats()

    fig, ax = plt.subplots()
    x = range(1,11,1)       # 10 points from the start image
    for i,j in enumerate(my_comparison_object.sal_vals_from_fixationsL): # i 0 - 10 for a total of 11 
        my_label = f"Image {i}"
        ax.plot(x,j, label = my_label)
    ax.legend()
    ax.set_xlabel('Ten Salient points from start image')
    ax.set_ylabel('Saliency values')
    ax.set_title('saliency variations from fixations')
    plt.show()


    nrow = 1
    ncol = 3
    fig, ax = plt.subplots(nrow, ncol)
    for index, i in enumerate([1,6,4]):
        img = my_comparison_object.imagesL[i]
        r,c = my_comparison_object.point_mapL[i][6]    # point 7 out of 10
        wide = 5
        img[r-wide:r+wide+1, c-wide:c+wide+1, 0] = 0    # RGB layers in the image
        img[r-wide:r+wide+1, c-wide:c+wide+1, 1] = 0
        img[r-wide:r+wide+1, c-wide:c+wide+1, 2] = 255
        my_label = f"image {i}"
        ax[index].imshow(img)
        ax[index].set_title(my_label)

    nrow = 1
    ncol = 3
    fig, ax = plt.subplots(nrow, ncol)
    for index, i in enumerate([0, 1, 2]):
        img = my_comparison_object.imagesL[i]
        r, c = my_comparison_object.point_mapL[i][6]  # point 7 out of 10
        wide = 5
        img[r - wide:r + wide + 1, c - wide:c + wide + 1, 0] = 0  # RGB layers in the image
        img[r - wide:r + wide + 1, c - wide:c + wide + 1, 1] = 0
        img[r - wide:r + wide + 1, c - wide:c + wide + 1, 2] = 255
        my_label = f"image {i}"
        ax[index].imshow(img)
        ax[index].set_title(my_label)


    fig, ax = plt.subplots()
    x = range(1, 12, 1)     # 11 images
    for i, j in enumerate(my_comparison_object.imagewise_variations_of_sal_pixelL):  # i 0 to 9 for a total of 10
        my_label = f"Point {i} across images"
        ax.plot(x, j, label=my_label)
    ax.legend()
    ax.set_xlabel('Ten Salient points from start image')
    ax.set_ylabel('Saliency values')
    ax.set_title('saliency variations from fixations')
    plt.show()

    fig, ax = plt.subplots()
    mylabel = f"Pixel 1 - Avg I"
    ax.plot(a[0][0], a[0][1], label = mylabel)
    mylabel = f"Pixel 1 - Avg Cb"
    ax.plot(a[10][0], a[10][1], label = mylabel)
    mylabel = f"Pixel 1 - Avg Cr"
    ax.plot(a[20][0], a[20][1], label = mylabel)
    ax.legend()
    ax.set_xlabel('Avg value of Intensity')
    ax.set_ylabel('Saliency')
    ax.set_title('saliency vs Avg YCB for salient pixel 1')
    plt.show()

    fig, ax = plt.subplots()
    mylabel = f"Pixel 1 - Std I"
    ax.plot(b[0][0], b[0][1], label=mylabel)
    mylabel = f"Pixel 1 - Std Cb"
    ax.plot(b[10][0], b[10][1], label=mylabel)
    mylabel = f"Pixel 1 - Std Cr"
    ax.plot(b[20][0], b[20][1], label=mylabel)
    ax.legend()
    ax.set_xlabel('STD value of Intensity')
    ax.set_ylabel('Saliency')
    ax.set_title('saliency vs Std. YCB for salient pixel 1')
    plt.show()


    nrow = 2
    ncol = 5
    fig, ax = plt.subplots(nrow,ncol)
    d = b[0:10]
    for i, j in enumerate(d):
        my_label = f"Saccade {i}"
        x = i//ncol
        y = i%ncol
        print(f"i: {i} - x: {x} - y: {y}")
        ax[x, y].plot(j[0], j[1], label=my_label)

    fig, ax = plt.subplots(1,2)
    x = np.linspace(1, 10, 10)
    my_points = my_comparison_object.saccade_pointsL
    my_points.insert(0, "Start image")
    for i,j in enumerate(my_comparison_object.saccade_variationsL):
        my_label = f"Saccade {i}"
        ax[0].plot(x, j, label=my_label)
    ax[0].legend()
    ax[0].set_xlabel('Avg value of Intensity')
    ax[0].set_ylabel('Saliency')
    ax[0].set_title('Variation of saliency')

    x = np.linspace(1, 11, 11)
    my_points = my_comparison_object.saccade_pointsL
    my_points.insert(0, "Start image")
    for i, j in enumerate(my_comparison_object.imagewise_variations_of_sal_pixelL):
        my_label = "Point {}".format(my_comparison_object.saccade_pointsL[i])
        ax[1].plot(x, j, label=my_label)
    ax[1].legend()
    ax[1].set_xlabel('saccades')
    ax[1].set_ylabel('Saliency')
    ax[1].set_title('Saliency of each Point for different saccades')
    plt.show()



    s = my_comparison_object.compute_image_stats()


    my_image_ensemble = sal_ensemble(image_ensemble)
    l_images, r_images = my_image_ensemble.get_images()
    l_sal_ensemble = []
    for i in l_images:
        if i is not None:
            l_sal_ensemble.append(1)
        else:
            l_sal_ensemble.append(None)
    r_sal_ensemble = []
    for i in r_images:
        if i is not None:
            r_sal_ensemble.append(2)
        else:
            r_sal_ensemble.append(None)

    a=9




    salval = read_pickled_file(salmap_file)
    my_sal_object = process_image(image_info_file, my_block = 16, total_points = 10, pixel_max = 150)
    my_sal_object.load_salmap(salval[0])
    my_sal_object.load_salmap(salval[1], "right")
    my_salfile = my_sal_object.save_salmap()
    my_sal_object.process_saliency(my_salfile)
    my_sal_object.save_all()

    '''
    my_sal_object.load_salmap(salval[0])
    my_sal_object.load_salmap(salval[1],"right")
    my_salfile = my_sal_object.save_salmap()

    my_sal_object.process_saliency(my_salfile)
    my_sal_object.save_all()
    '''
    read_sal_obj = my_sal_object.read_sal_datafile()


    """
        fig = plt.figure(figsize=(8, 8))
        r1c1 = fig.add_subplot(2, 2, 1)
        r1c2 = fig.add_subplot(2, 2, 2)
        r2c1 = fig.add_subplot(2, 2, 3)
        r2c2 = fig.add_subplot(2, 2, 4)
        r1c1.imshow(my_sal_object.image)
        r1c2.imshow(my_sal_object.salmap)
        r2c1.imshow(my_sal_object.reduced_salmap)
        r2c2.imshow(my_sal_object.recreated_salmap)
        plt.show()
    """

    fig1 = plt.figure(figsize=(8, 8))
    r1c1 = fig1.add_subplot(2, 2, 1)
    r1c2 = fig1.add_subplot(2, 2, 2)
    r2c1 = fig1.add_subplot(2, 2, 3)
    r2c2 = fig1.add_subplot(2, 2, 4)
    r1c1.imshow(read_sal_obj[0][0])
    r1c2.imshow(read_sal_obj[1][0])
    r2c1.imshow(read_sal_obj[2][0])
    r2c2.imshow(read_sal_obj[3][0])


    fig2 = plt.figure(figsize=(8, 8))
    r1c1 = fig2.add_subplot(2, 2, 1)
    r1c2 = fig2.add_subplot(2, 2, 2)
    r2c1 = fig2.add_subplot(2, 2, 3)
    r2c2 = fig2.add_subplot(2, 2, 4)
    r1c1.imshow(read_sal_obj[0][1])
    r1c2.imshow(read_sal_obj[1][1])
    r2c1.imshow(read_sal_obj[2][1])
    r2c2.imshow(read_sal_obj[3][1])
    plt.show()
    # agent orientation, agent position, lefteye Rotation, list of x,y points, righteye Rotation, list of x,y points
    sdata = get_salpoints(image_info_file + "-sal-processed")
    print(f"Head Orientation {sdata[0]} and Pos  {sdata[1]} and Head-Neck rotation {sdata[2]}")
    print(f"Left Eye orientation {sdata[4]} and the left eye image salient points:")
    for count, sval in enumerate(sdata[5]):
        print(f"{count}. {sval}")
    print(f"Right eye orientation {sdata[7]} and the right eye image salient points:")
    for count, sval in enumerate(sdata[8]):
        print(f"{count}. {sval}")

    a=5



