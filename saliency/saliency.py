import pickle
import numpy as np
import skimage.measure
import math
import quaternion
import matplotlib.pyplot as plt

''' Processing instruction that assumes the use of colab to run Deep Gaze II.
    For processing Deep Gaze output,
    1.  - A start image file scene^timestamp has a unique timestamp and view of the oreo habitat scene as seen by oreo
        (van-gogh-room.glb^2021-05-09-22-37)
    2.  - create a sal_object by invoking the process_image with file location of the starting image.
        It has the start left and right images but no saliency map for them
    3.  Use Deep Gaze II to produce the saliency map for the starting image L R images
    4.  Invoke the save_salmap(ndarray of 2D saliency values) to create "-sal" file containing just the saliency map
    5.  call process_saliency without -sal file if you are doing it right after save_salmap
        This processes the file, generates 10 points and contains the images of the processed salmap files
    6.  calling save_all saves a "-sal-processed"
    7.  Invoking capture_images_for_salpoints("-sal-processed" file) in Habitat_oreo object generates an image-ensemble file.
        that ends with "-sal-processed-images". It is generated by using the get_salpoints("-sal-processed file") method
        in saliency.py to iterate through the points and capture images of the same oreo habitat scene with the gaze vectors
        in the direction of the point. At present the initial L and R images is not fixated.
    8.  create a sal-ensemble object ("-sal-processed-images" file) and use Deep Gaze to obtain salmap for each image.
    9.  save_sal_ensemble(left_map_list, right_map_list) creates  "-sal-processed-images-sal-ensemble" file containing
        all the saliency maps.

'''

image_info_file = "./saliency_map/van-gogh-room.glb^2021-05-09-22-47"
salmap_file = "./saliency_map/van-gogh-room.glb^2021-05-09-22-37-sal"



def scale_image(some_image, new_max = 150):
    vmin = some_image.min()
    vmax = some_image.max()
    some_image = (some_image - vmin) * new_max / (vmax - vmin)
    return some_image


def find_max_and_index(array_2d):
    #result = np.where(array_2d == np.amax(array_2d))
    result = np.nonzero(array_2d == np.amax(array_2d))
    r = result[0][0]
    c = result[1][0]
    return np.amax(array_2d), r, c


def get_salpoints(processed_sal_file):
    '''
    :parameter processed_sal_file - A processed saliency file, name ending with "-sal-processed".
    It would contain a maximum of 10 salient points identified in the saliency heat map.
    This function can be used to read such a file.
    :returns agent orientation, agent Position, robot_head_neck_rotation,
    left_image, lefteye Rotation, list of x,y points, right_image, righteye Rotation, list of x,y points
    '''
    try:
        with open(processed_sal_file, "rb") as f:
            saldata = pickle.load(f)
            sal_pL = []
            sal_pR = []
            for i in saldata[4][0]:
                sal_pL.append(i[2:])
            for i in saldata[4][1]:
                sal_pR.append(i[2:])

            return saldata[6], saldata[7], saldata[9], saldata[0][0], saldata[8][0], sal_pL, saldata[0][1], \
                   saldata[8][1], sal_pR
    except:
        print(f"Failure: To read saliency processed datafile {processed_sal_file}")
        return None

def read_pickled_file(sfilename, filetype="-"):
    '''
    :parameter - sfilename is a filename
    '''
    try:
        with open(sfilename, "rb") as f:
            pickled_data = pickle.load(f)
            return pickled_data
    except:
        print(f"Failure: To read {filetype} file {sfilename}")
        return None

def compute_pixel_in_current_frame(R1, R2, pixels_in_previous_frame, focal_distance, width, height):
    '''
    Reference for rotation is Habitat World Coordinates
    :param R1 Rotation in quaterion from WCS to previous sensor(eye) frame
    :param R2 Rotation in quaterion from WCS to current sensor(eye) frame
    :param pixels_in_previous_frame: List of x,y positions of pixel in the previous frame
    :param focal_distance: the frame z coordinate is equal to -focal_distance
    :param width: sensor width pixels
    :param height: sensor height pixels
    :returns List of x, y positions in the current frame
    '''

    R = R2.inverse() * R1  # Rotation from current frame to previous frame
    w = width / 2
    h = height / 2
    new_list = []
    for i in pixels_in_previous_frame:  # i should (x,y)
        # shifting based on an origin at the center of the frame and computing the unit vector
        x = i[0] - w
        y = h - i[1]
        v = np.array([x, y, -focal_distance])
        uvector = v / np.linalg.norm(v)

        # rotate the uvector
        new_vector = quaternion.as_rotation_matrix(R).dot(uvector.T)

        ux = new_vector[0]
        uy = new_vector[1]
        uz = new_vector[2]
        # calculate angles that the unit vector makes with z axis and with xz plane
        uxz = np.sqrt(ux * ux + uz * uz)
        theta = np.arcsin(ux / uxz)  # z is never zero, theta is the rotation angle about y-axis - yaw angle
        phi = np.arcsin(uy)  # x is the angle about x - pitch angle
        # compute x,y (z = -focal length)
        xval = focal_distance * np.tan(theta)
        yval = focal_distance * np.tan(phi)
        # convert to top left origin
        xn = math.floor(xval + w)
        yn = math.floor(h - yval)
        #if xn <= width and yn <= height:    # logic is suspect;
        if 0 <= xn <= width and 0<= yn <= height:  # logic is suspect;
            pos = (xn, yn)
            #combo = (i, pos)
            new_list.append(pos)
        else:
            new_list.append(None)
            print(f"point {i}in old frame is outside the new frame at {xn},{yn}")
    return new_list


class process_image(object):
    '''
    Since salmap comes from Deep Gaze II through collab, saving the salmap is not in the __init__.
    Creating a process image object will allow us to run Deep Gaze II without running habitat-sim in collab.
    Once created, use save_salmap to save the saliency maps of from the left and right images to create a -sal file.
    Its methods can also be used to create a "-processed" which will contain image, salmap, centerpoints, and
    rotation information for both images.
    '''

    def __init__(self, image_info_file, my_block = 16, total_points = 10, pixel_max = 150):
        self.fname = image_info_file
        try:
            with open(image_info_file, "rb") as f:
                scene_data = pickle.load(f)
        except IOError as e:
            print("Failure: Loading Image pickle file {}".format(image_info_file))
            exit(1)

        '''    
        scene_data
            0 - Agent orn
            1 - Agent position
            2 - robot_head_neck_rotation
            3 - lefteye orientation
            4 - righteye orientation
            5 - left_sensor.resolution
            6 - left_sensor_hfov
            7 - focal_distance
            8 - images[0 = left, 1 = right, 2 = depth optionally set]
        '''
        self.imageL = scene_data[8][0][:, :, ::-1]  # Left image
        self.imageR = scene_data[8][1][:, :, ::-1]  # Right image
        self.lefteye = scene_data[3]    # Left sensor rotation
        self.righteye = scene_data[4]   # right sensor rotation
        self.agent_orn = scene_data[0]  # Agent orn
        self. agent_pos = scene_data[1]  # Agent position
        self.hfov = scene_data[6]
        self.focal_distance = scene_data[7]
        self.robot_head_neck_rotation = scene_data[2] #head-neck rotation that is buried into the Agent orn.
        self.block_dim = my_block
        self.num_points = total_points
        self.new_max = pixel_max
        #Saliency - left image
        self.salmapL = None
        self.reduced_salmapL = None
        self.recreated_salmapL = None
        self.reduced_sal_pointsL = []
        self.center_pointsL = []
        #Saliency - right image
        self.salmapR = None
        self.reduced_salmapR = None
        self.recreated_salmapR = None
        self.reduced_sal_pointsR = []
        self.center_pointsR = []

        self.output_filename = image_info_file + "-sal-processed"
        self.salmap_filename = image_info_file + "-sal"

    def load_salmap(self,salval, which_eye = "left"):
        '''
        Loads the individual saliency heat map for each eye into the process object for an image file
        created by agent_oreo.py
        :param salval: saliency map as numpy ndarray, from Deep Gaze II output
        Use this to save both the right and left image saliency maps from Deep Gaze II output
        '''

        if which_eye == "right":
            self.salmapR = salval
        else:
            self.salmapL = salval

    def save_salmap(self):
        '''
        Use this to save the right and left saliency values from Deep Gaze II for a given right and left image pair
        '''

        both_salmap = [self.salmapL, self.salmapL]
        try:
            with open(self.salmap_filename, "wb") as f:
                pickle.dump(both_salmap, f)
                return self.salmap_filename
        except:
            print(f"Failure: To open and save saliency file {self.salmap_filename}")


    def process_saliency(self, sal_file = None):
        '''
        :parameter block_dim - size reduction of original image for identifying salient points
        :parameter num_points - number of salient points to be identified.
        It creates reduced_salmap, recreated_salmap, and list of salient points.
        '''
        if sal_file is not None:     # salmap available as a file to run outside of colab
            try:
                with open(sal_file, "rb") as f:
                    salmaps = pickle.load(f)
                    self.salmapL = salmaps[0]
                    self.salmapR = salmaps[1]
            except IOError as e:
                print(f"Failure: Loading pickle saliency map file {sal_file}")
                return

        # This portion would also work without sal_file immediately after saving the Deep Gaze II.
        if self.salmapL is None and self.salmapR is None:
            print(print(f"Failure: Saliency numpy ndarray is empty"))
            return
        else:
            # self.salmap[0] = left saliency map and self.salmap[1] = right saliency map
            # Macular angle is 18 out of HFOV (120); d should be 5
            # We ignore (d-1)/2 pixels on horizontal and vertical directions of a salient pixel
            # When dim = 16x16 for map reduction, d=5 means 2 pixels on each side, 32 pixels in 512 scale
            # Each red. image pixel = 16 regular approximately 3.5 degrees
            hfov = np.degrees(self.hfov)
            self.d = math.ceil((18.0 * self.salmapL.shape[0]) / (hfov * self.block_dim))
            # print(f"The dimension d = {self.d}")
            # compute salient points list
            f = int((self.d - 1) / 2)
            if self.salmapL is not None:
                self.compute_points_and_maps(f)
            if self.salmapR is not None:
                self.compute_points_and_maps(f,"right")


    def compute_points_and_maps(self, f, which_map = "left"):
        # Macular angle is 18 out of HFOV (120); d should be 5
        # We ignore (d-1)/2 pixels on horizontal and vertical directions of a salient pixel
        # When dim = 16x16 for map reduction, d=5 means 2 pixels on each side, 32 pixels in 512 scale
        # Each red. image pixel = 16 regular approximately 3.5 degrees

        if which_map == "right":
            raw_salmap = self.salmapR
        else:
            raw_salmap = self.salmapL

        reduced_salmap = skimage.measure.block_reduce(raw_salmap, block_size=(self.block_dim,
                                                                                      self.block_dim), func=np.amax)
        reduced_salmap = scale_image(reduced_salmap)
        reduced_sal_points = []
        for i in range(self.num_points):
            val, r, c = find_max_and_index(reduced_salmap)
            reduced_sal_points.append((val, r, c))
            reduced_salmap[r - f:r + f + 1, c - f:c + f + 1] = 0  # zero_around_macular_center

        for i in range(self.num_points):
            reduced_salmap[reduced_sal_points[i][1], reduced_sal_points[i][2]] = 255 - i * 10

        new_sal_image = np.copy(raw_salmap)
        vmin = new_sal_image.min()
        vmax = new_sal_image.max()
        recreated_salmap = (new_sal_image - vmin) * self.new_max / (vmax - vmin)

        count = 0
        center_points = []
        for scaled_val, r, c in reduced_sal_points:
            start_r = r * self.block_dim
            start_c = c * self.block_dim
            '''
            recreated_image[start_r:start_r+block_size,start_c:start_c+block_size] = \
                recreated_image[start_r:start_r + block_size, start_c:start_c + block_size]/2
            '''
            val = 255 - 10 * count
            recreated_salmap[start_r:start_r + self.block_dim, start_c:start_c + self.block_dim] = val
            true_val = raw_salmap[start_r + 16, start_c + 16]
            center_points.append((true_val, scaled_val, start_r + 16, start_c + 16))
            count += 1

        if which_map == "left":
            self.reduced_salmapL = reduced_salmap
            self.recreated_salmapL = recreated_salmap
            self.center_pointsL = center_points
            self.reduced_sal_pointsL = reduced_sal_points

        else:
            self.reduced_salmapR = reduced_salmap
            self.recreated_salmapR = recreated_salmap
            self.center_pointsR = center_points
            self.reduced_sal_pointsR = reduced_sal_points



    def save_all(self):
        '''
        After processing, call this to save a -sal-processed file which contains both images, salicency heat maps,
        list of salient points, and the associated posn orientation of the eye sensors.
        '''
        all = []
        image = [self.imageL, self.imageR]
        all.append(image)
        salmap = [self.salmapL,self.salmapR]
        all.append(salmap)
        reduced_salmap = [self.reduced_salmapL,self.reduced_salmapR]
        all.append(reduced_salmap)
        recreated_salmap = [self.recreated_salmapL, self.recreated_salmapR]
        all.append(recreated_salmap)
        center_points = [self.center_pointsL, self.center_pointsR]
        all.append(center_points)
        all.append(self.focal_distance)
        all.append(self.agent_orn)
        all.append(self.agent_pos)
        rotation = [self.lefteye, self.righteye]
        all.append(rotation)
        all.append(self.robot_head_neck_rotation)

        try:
            with open(self.output_filename, "wb") as f:
                pickle.dump(all, f)
                print(f"Saved processed saliency file {self.output_filename}")
        except:
            print(f"Failure: To save saliency file {self.output_filename}")


    def read_sal_datafile(self):
        '''
        reads the processed file saved by process_image object
        :return: List or None.
            0 - image L, R
            1 - salmap L, R
            2 - reduced_salmap L, R
            3 - recreated_salmap L, R
            4 - center_points L , R
            5 - focal_distance
            6 - agent_orn
            7 - agent_pos
            8 - rotation L, R
            9 - robot_head_neck_rotation
        '''
        try:
            with open(self.output_filename, "rb") as f:
                saldata = pickle.load(f)
                return saldata
        except:
            print(f"Failure: To read saliency processed datafile {self.output_filename}")
            return None


class sal_ensemble(object):

    def __init__(self, image_ensemble_file):
        try:
            with open(image_ensemble_file, "rb") as f:
                data = pickle.load(f)
        except IOError as e:
            print("Failure: Loading pickle file {}".format(image_ensemble_file))
            exit(1)

        self.imgL_data = data[0]
        self.imgR_data = data[1]
        self.sal_ensemble_filename = image_ensemble_file + "-sal-ensemble"

    def get_images(self):

        left_images = []
        for i in self.imgL_data:
            if i[1] is not None:
                left_images.append(i[1][4])
            else:
                left_images.append(None)

        right_images = []
        for i in self.imgR_data:
            if i[1] is not None:
                right_images.append(i[1][4])
            else:
                right_images.append(None)
        return left_images, right_images

    def save_sal_ensemble(self, left_salmaps, right_salmaps):
        output = [left_salmaps,right_salmaps]
        try:
            with open(self.sal_ensemble_filename, "wb") as f:
                pickle.dump(output, f)
                print(f"Saved new saliency file {self.sal_ensemble_filename}")
        except IOError as e:
            print(f"Failure: To open/write file {self.sal_ensemble_filename}")


class saccade_comparison(object):

    def __init__(self, start_image_processed_filename, image_ensemble_filename, sal_ensemble_filename):
        '''
        Corresponding pixels for saccades requires Starting_R, current_R, ref_point_list, focal_distance, w, h
        start_image_processed_file contains the following list
        [0] - images - [L, R]
        [1] - salmaps - [L, R]
        [3] - recreated_salmap [L, R]
        [4] - List of points [L, R]
        [5] - focal_distance
        [6] - agent_orn
        [7] - agent_pos
        [8] - rotation [lefteye, righteye]
        [9] - robot_head_neck_rotation
        Image ensemble is a list = [L,R] L/R is [gaze_point, [aorn, apos, agent_head_neck_rotation, l_sensor_orn, image]]
        Sal ensemble is a list =[L,R] l/R is a list of salmaps
        '''

        self.start_data = read_pickled_file(start_image_processed_filename, "processed start image file")
        if self.start_data is None:
             exit(1)
        d = start_image_processed_filename.find("-sal-processed")
        self.saccade_variations_filename = start_image_processed_filename[0:d] + "-saccade_variations"

        self.start_sensor_rotation = self.start_data[8]     #Reference Rotation [L,R] of start image
        self.focal_distance = self.start_data[5]
        self.salmap_dim = self.start_data[1][0].shape       #left salmap is used to get (w,h)

        self.saccade_pointsL = []
        for i in self.start_data[4][0]:
            self.saccade_pointsL.append((i[2:]))
        self.saccade_pointsR = []
        for i in self.start_data[4][1]:
            self.saccade_pointsR.append((i[2:]))

        self.start_salmap_L = self.start_data[1][0]         #start image saliency heat map of Left
        self.start_salmap_R = self.start_data[1][1]         #start image saliency heat map of Right

        self.image_ensemble = read_pickled_file(image_ensemble_filename, "image ensemble file")
        if self.image_ensemble is None:
            exit(1)
        self.saccade_rotationsL = []
        for i in self.image_ensemble[0]:                    #Left
            if i[1] is None:                                #None data for the gaze point
                self.saccade_rotationsL.append(None)
            else:
                self.saccade_rotationsL.append(i[1][3])     #sensor rotation
        self.saccade_rotationsR = []
        for i in self.image_ensemble[1]:                    #right
            if i[1] is None:                                #None data for the gaze point
                self.saccade_rotationsR.append(None)
            else:
                self.saccade_rotationsR.append(i[1][3])     #sensor rotation

        self.sal_ensemble = read_pickled_file(sal_ensemble_filename, "saliency ensemble file")
        if self.sal_ensemble is None:
            exit(1)
        # left saliency heatmap = self.sal_ensemble[0], right saliency heatmap = self.sal_ensemble[1]
        self.sal_ensemble[0].insert(0,self.start_data[1][0])    #salmap of the Left start image inserted to 0th position
        self.sal_ensemble[1].insert(0, self.start_data[1][1])   #salmap of the right start image inserted to 0th position

        self.point_mapL = []
        self.point_mapR = []
        self.saccade_variationsL = []
        self.saccade_variationsR = []

    def build_point_map(self):
        '''
        It creates a n x n List of List. Each inner list is a set of pixel locations. The first list is the salient pixels
        location from the start image. The second list is the location in the second's top left 0,0 coordinate system,
        the locations of the points after eye has saccaded to the second salient pixel in the start image. Therefore every
        list (from 1 to 10) will have 256,256 point or close enough in tis list. A none list means the eye could not saccade and
        a none value means the point was outse the 512x512 frame.
        '''

        self.point_mapL.append(self.saccade_pointsL)
        for i in self.saccade_rotationsL:
            if i is not None:
                point_set = compute_pixel_in_current_frame(self.start_sensor_rotation[0], i, self.saccade_pointsL,
                                                           self.focal_distance, self.salmap_dim[0], self.salmap_dim[1])
                self.point_mapL.append(point_set)
            else:
                my_list = [None] * len(self.saccade_pointsL)
                self.point_mapL.append(my_list)

        self.point_mapR.append(self.saccade_pointsR)
        for i in self.saccade_rotationsR:
            if i is not None:
                point_set = compute_pixel_in_current_frame(self.start_sensor_rotation[0], i, self.saccade_pointsR,
                                                           self.focal_distance, self.salmap_dim[0], self.salmap_dim[1])
                self.point_mapR.append(point_set)
            else:
                my_list = [None] * len(self.saccade_pointsR)
                self.point_mapR.append(my_list)
        return

    def compute_saccade_sal_variations(self):

        for i, point_list in enumerate(self.point_mapL):
            sal_values =[]
            for loc in point_list:
                    if loc is None:
                        sal_values.append(None)
                    else:
                        sal_values.append(self.sal_ensemble[0][i][loc[0],loc[1]])
            self.saccade_variationsL.append(sal_values)

        for i, point_list in enumerate(self.point_mapR):
            sal_values = []
            for loc in point_list:
                if loc is None:
                    sal_values.append(None)
                else:
                    sal_values.append(self.sal_ensemble[1][i][loc[0], loc[1]])
            self.saccade_variationsR.append(sal_values)

        try:
            with open(self.saccade_variations_filename, "wb") as f:
                pickle.dump([self.saccade_variationsL, self.saccade_variationsR], f)
        except:
            print(f"Failure: To open and save saccade variations file {self.saccade_variations_filename}")
        return

if __name__ == "__main__":

    start_processed = "./saliency_map/van-gogh-room.glb^2021-05-09-22-47-sal-processed"
    image_ensemble = "./saliency_map/van-gogh-room.glb^2021-05-09-22-47-sal-processed-images"
    sal_ensemble = "./saliency_map/van-gogh-room.glb^2021-05-09-22-47-sal-processed-images-sal-ensemble"

    my_comparison_object = saccade_comparison(start_processed, image_ensemble, sal_ensemble)
    my_comparison_object.build_point_map()
    my_map = my_comparison_object.point_mapL
    my_comparison_object.compute_saccade_sal_variations()
    my_sal_variations = my_comparison_object.saccade_variationsL



    my_image_ensemble = sal_ensemble(image_ensemble)
    l_images, r_images = my_image_ensemble.get_images()
    l_sal_ensemble = []
    for i in l_images:
        if i is not None:
            l_sal_ensemble.append(1)
        else:
            l_sal_ensemble.append(None)
    r_sal_ensemble = []
    for i in r_images:
        if i is not None:
            r_sal_ensemble.append(2)
        else:
            r_sal_ensemble.append(None)

    a=9




    salval = read_pickled_file(salmap_file)
    my_sal_object = process_image(image_info_file, my_block = 16, total_points = 10, pixel_max = 150)

    '''
    my_sal_object.load_salmap(salval[0])
    my_sal_object.load_salmap(salval[1],"right")
    my_salfile = my_sal_object.save_salmap()

    my_sal_object.process_saliency(my_salfile)
    my_sal_object.save_all()
    '''
    read_sal_obj = my_sal_object.read_sal_datafile()


    """
        fig = plt.figure(figsize=(8, 8))
        r1c1 = fig.add_subplot(2, 2, 1)
        r1c2 = fig.add_subplot(2, 2, 2)
        r2c1 = fig.add_subplot(2, 2, 3)
        r2c2 = fig.add_subplot(2, 2, 4)
        r1c1.imshow(my_sal_object.image)
        r1c2.imshow(my_sal_object.salmap)
        r2c1.imshow(my_sal_object.reduced_salmap)
        r2c2.imshow(my_sal_object.recreated_salmap)
        plt.show()
    """

    fig1 = plt.figure(figsize=(8, 8))
    r1c1 = fig1.add_subplot(2, 2, 1)
    r1c2 = fig1.add_subplot(2, 2, 2)
    r2c1 = fig1.add_subplot(2, 2, 3)
    r2c2 = fig1.add_subplot(2, 2, 4)
    r1c1.imshow(read_sal_obj[0][0])
    r1c2.imshow(read_sal_obj[1][0])
    r2c1.imshow(read_sal_obj[2][0])
    r2c2.imshow(read_sal_obj[3][0])


    fig2 = plt.figure(figsize=(8, 8))
    r1c1 = fig2.add_subplot(2, 2, 1)
    r1c2 = fig2.add_subplot(2, 2, 2)
    r2c1 = fig2.add_subplot(2, 2, 3)
    r2c2 = fig2.add_subplot(2, 2, 4)
    r1c1.imshow(read_sal_obj[0][1])
    r1c2.imshow(read_sal_obj[1][1])
    r2c1.imshow(read_sal_obj[2][1])
    r2c2.imshow(read_sal_obj[3][1])
    plt.show()
    # agent orientation, agent position, lefteye Rotation, list of x,y points, righteye Rotation, list of x,y points
    sdata = get_salpoints(image_info_file + "-sal-processed")
    print(f"Head Orientation {sdata[0]} and Pos  {sdata[1]} and Head-Neck rotation {sdata[2]}")
    print(f"Left Eye orientation {sdata[4]} and the left eye image salient points:")
    for count, sval in enumerate(sdata[5]):
        print(f"{count}. {sval}")
    print(f"Right eye orientation {sdata[7]} and the right eye image salient points:")
    for count, sval in enumerate(sdata[8]):
        print(f"{count}. {sval}")

    a=5



